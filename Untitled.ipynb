{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "emotional-disposition",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dapaleev/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch.ao'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9070d4d9fd0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HAS_OPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mswin_transformer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmaxvit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdetection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptical_flow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegmentation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_model_builder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_model_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/models/quantization/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgooglenet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0minception\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmobilenet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mshufflenetv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/models/quantization/mobilenet.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmobilenetv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401, F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmobilenetv3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401, F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmobilenetv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmv2_all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmobilenetv3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmv3_all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/models/quantization/mobilenetv2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeQuantStub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQuantStub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmobilenetv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInvertedResidual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMobileNet_V2_Weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMobileNetV2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch.ao'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-calgary",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gt = pd.read_csv(\"/home/dapaleev/facial_points/facial_points/public_tests/00_test_img_input/train/gt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-aquatic",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-chrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gt[train_gt['filename'] == '00000.jpg'].iloc[:, 1:].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-wallace",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacialPointsDataset(Dataset):\n",
    "    SPLIT_RANDOM_SEED = 42\n",
    "    TEST_SIZE = 0.25\n",
    "    \n",
    "    def __init__(self, root, train_gt, train=True, load_to_ram=True, transform=None):\n",
    "        super().__init__()\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.load_to_ram = load_to_ram\n",
    "        self.transform = transform\n",
    "        self.to_tensor = T.ToTensor()\n",
    "        self.all_files = []\n",
    "        self.all_labels = []\n",
    "        self.images = []\n",
    "        \n",
    "        \n",
    "        files = sorted(os.listdir(self.root))\n",
    "        train_files, test_files = train_test_split(files, random_state=self.SPLIT_RANDOM_SEED, test_size=self.TEST_SIZE)\n",
    "        \n",
    "        print()\n",
    "        if self.train :\n",
    "            files = train_files\n",
    "        else:\n",
    "            files = test_files\n",
    "        self.all_files = files\n",
    "        for filename in tqdm(files):\n",
    "            if self.load_to_ram:\n",
    "                image = Image.open(os.path.join(self.root, filename)).convert('RGB')\n",
    "                self.images += [image]\n",
    "            self.all_labels += [torch.from_numpy(train_gt[train_gt['filename'] == filename].iloc[:, 1:].to_numpy()[0])]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_files)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        label = self.all_labels[item]\n",
    "        if self.load_to_ram:\n",
    "            image = self.images[item]\n",
    "        else:\n",
    "            filename = self.all_files[item]\n",
    "            image = Image.open(os.path.join(self.root, self.classes[label], filename)).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "test_transform = T.Compose([\n",
    "    T.Resize(90),\n",
    "    T.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-conditioning",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"/home/dapaleev/facial_points/facial_points/public_tests/00_test_img_input/train/images\"\n",
    "train_dataset = FacialPointsDataset(train_dir, train_gt, transform=test_transform)\n",
    "test_dataset = FacialPointsDataset(train_dir, train_gt, train = False, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-oxygen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-license",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = train_dataset[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-montgomery",
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-group",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_keypoints(image, label):\n",
    "    plt.imshow(image.resize(, cmap='gray')\n",
    "    plt.plot(label[0::2], label[1::2], 'gx')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-accommodation",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_keypoints(*train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-promotion",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-aerospace",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, pin_memory=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, pin_memory=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-darwin",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-circle",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-diesel",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out_features = 28\n",
    "\n",
    "class BasicBlockNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding='same'), \n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding='same'), \n",
    "            nn.BatchNorm2d(32)\n",
    "        )\n",
    "        self.conv1_skip = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=1)\n",
    "        \n",
    "        self.conv2 = nn.Sequential(  \n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "        self.conv2_skip = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=1)\n",
    "        \n",
    "        self.conv3 = nn.Sequential(  \n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "        self.conv3_skip = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=1)\n",
    "        \n",
    "        self.conv4 = nn.Sequential(  \n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(256)\n",
    "        )\n",
    "        self.conv4_skip = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=1)\n",
    "        \n",
    "        self.conv5 = nn.Sequential(  \n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(512)\n",
    "        )\n",
    "        self.conv5_skip = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=1)\n",
    "        \n",
    "        self.max_pool = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(in_features=4608, out_features=512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(in_features=4608, out_features=out_features)\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.max_pool(self.conv1(x) + self.conv1_skip(x)) #3 x 96 x 96 --> 32 x 48 x 48\n",
    "        x = self.max_pool(self.conv2(x) + self.conv2_skip(x)) #32 x 48 x 48 --> 64 x 24 x 24\n",
    "        x = self.max_pool(self.conv3(x) + self.conv3_skip(x)) #64 x 24 x 24 --> 128 x 12 x 12\n",
    "        x = self.max_pool(self.conv4(x) + self.conv4_skip(x)) #128 x 12 x 12 --> 256 x 6 x 6\n",
    "        x = self.max_pool(self.conv5(x) + self.conv5_skip(x)) #256 x 6 x 6 --> 512 x 3 x 3\n",
    "        x = x.reshape(x.shape[0], 4608)\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-snowboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def test(model, loader):\n",
    "    loss_log = []\n",
    "    model.eval()\n",
    "    \n",
    "    for data, target in loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        logits = model(data)\n",
    "\n",
    "        loss = criterion(logits, target)\n",
    "        loss_log.append(loss.item())\n",
    "        \n",
    "    return np.mean(loss_log)\n",
    "\n",
    "def train_epoch(model, optimizer, train_loader):\n",
    "    loss_log = []\n",
    "    model.train()\n",
    "    \n",
    "    for data, target in train_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(data)\n",
    "        loss = criterion(logits, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_log.append(loss.item())\n",
    "\n",
    "    return loss_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-frost",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(model, optimizer, n_epochs, train_loader, val_loader, scheduler=None):\n",
    "    train_loss_log, train_acc_log, val_loss_log, val_acc_log, train_loss_epoch, train_acc_epoch = [], [], [], [], [], []\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        train_loss = train_epoch(model, optimizer, train_loader)\n",
    "        val_loss = test(model, val_loader)\n",
    "        \n",
    "        train_loss_log.extend(train_loss)\n",
    "        \n",
    "        val_loss_log.append(val_loss)\n",
    "\n",
    "        train_loss_epoch.append(np.mean(train_loss))\n",
    "\n",
    "    \n",
    "        print(f\"Epoch {epoch}\")\n",
    "        print(f\" train loss: {np.mean(train_loss)}, val loss: {val_loss}\")\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "    return train_loss_log, train_acc_log, val_loss_log, val_acc_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-coral",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = BasicBlockNet().to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.1)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',verbose=True, patience=5)\n",
    "train_loss_log, val_loss_log = train(net, optimizer, 90, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-heaven",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-north",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-collins",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-snapshot",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-smile",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
